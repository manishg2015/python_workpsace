{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:34.829642Z",
     "start_time": "2019-08-12T11:37:32.958610Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hmA6EzkQJ5jt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "training_size=160000\n",
    "# test_portion=.1\n",
    "\n",
    "# corpus = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:23:14.902469Z",
     "start_time": "2019-08-12T09:22:28.146035Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "bM0l_dORKqE0",
    "outputId": "4314f0d4-80a6-4250-8f44-a8b9c0ce4a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_IN.\n",
      "Warning: Failed to set locale category LC_TIME to en_IN.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_IN.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_IN.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_IN.\n",
      "--2019-08-12 14:52:28--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.197.80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.197.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 238942690 (228M) [application/octet-stream]\n",
      "Saving to: ‘./training_cleaned.csv’\n",
      "\n",
      "./training_cleaned. 100%[===================>] 227.87M  8.72MB/s    in 45s     \n",
      "\n",
      "2019-08-12 14:53:14 (5.03 MB/s) - ‘./training_cleaned.csv’ saved [238942690/238942690]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n",
    "# You can do that yourself with:\n",
    "# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n",
    "# I then hosted it on my site to make it easier to use in this notebook\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
    "    -O ./training_cleaned.csv\n",
    "\n",
    "# num_sentences = 0\n",
    "\n",
    "# with open(\"/tmp/training_cleaned.csv\") as csvfile:\n",
    "#     reader = csv.reader(csvfile, delimiter=',')\n",
    "#     for row in reader:\n",
    "#         list_item=[]\n",
    "#         list_item.append(row[5])\n",
    "#         this_label=row[0]\n",
    "#         if this_label=='0':\n",
    "#             list_item.append(0)\n",
    "#         else:\n",
    "#             list_item.append(1)\n",
    "#         num_sentences = num_sentences + 1\n",
    "#         corpus.append(list_item)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:36.772315Z",
     "start_time": "2019-08-12T11:37:36.769540Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3kxblBUjEUX-",
    "outputId": "8727edae-7fce-4f3c-faa2-692a084cbd07"
   },
   "outputs": [],
   "source": [
    "# print(num_sentences)\n",
    "# print(len(corpus))\n",
    "# print(corpus[1])\n",
    "\n",
    "# Expected Output:\n",
    "# 1600000\n",
    "# 1600000\n",
    "# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:41.257509Z",
     "start_time": "2019-08-12T11:37:37.396856Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('training_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:41.272276Z",
     "start_time": "2019-08-12T11:37:41.259390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:42.483036Z",
     "start_time": "2019-08-12T11:37:42.479087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding colun names to the data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:44.400951Z",
     "start_time": "2019-08-12T11:37:43.846435Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data =pd.DataFrame(data.values, columns = [\"label\", \"Data1\", \"Data2\", \"Data3\", \"Data4\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:45.154528Z",
     "start_time": "2019-08-12T11:37:44.696525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      "label    1599999 non-null object\n",
      "Data1    1599999 non-null object\n",
      "Data2    1599999 non-null object\n",
      "Data3    1599999 non-null object\n",
      "Data4    1599999 non-null object\n",
      "text     1599999 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "labeled_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:45.805584Z",
     "start_time": "2019-08-12T11:37:45.754713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_data.iloc[:, 0].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:46.467405Z",
     "start_time": "2019-08-12T11:37:46.416420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**convert first column as lable with values 0 or 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:48.382653Z",
     "start_time": "2019-08-12T11:37:47.668108Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data.iloc[:, 0] = labeled_data.iloc[:, 0].apply(lambda x : 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:48.725741Z",
     "start_time": "2019-08-12T11:37:48.713614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping unused columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:50.518955Z",
     "start_time": "2019-08-12T11:37:50.404462Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data.drop(['Data1','Data2','Data3','Data4'], inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:37:51.235000Z",
     "start_time": "2019-08-12T11:37:51.230719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:44:38.495967Z",
     "start_time": "2019-08-12T11:44:38.490513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manishgarg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('...')\n",
    "stop_words.append('.')\n",
    "stop_words.append('!')\n",
    "stop_words.append('@')\n",
    "stop_words.append('?')\n",
    "stop_words.append(';')\n",
    "stop_words.append('&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:44:39.691012Z",
     "start_time": "2019-08-12T11:44:39.688602Z"
    }
   },
   "outputs": [],
   "source": [
    "##labeled_data['text'] = labeled_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:26.622848Z",
     "start_time": "2019-08-12T11:44:45.181698Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data['text'] = labeled_data['text'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:53.581283Z",
     "start_time": "2019-08-12T11:47:40.099113Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data.to_csv(\"./training_cleaned_updated.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:53.587199Z",
     "start_time": "2019-08-12T11:47:53.583760Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = labeled_data['text'].values\n",
    "ys = labeled_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:54.530263Z",
     "start_time": "2019-08-12T11:47:53.589850Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_trainable, y_train, y_trainable = train_test_split(xs, ys,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=101,\n",
    "                                                    stratify=ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:54.535642Z",
     "start_time": "2019-08-12T11:47:54.532078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:56.433315Z",
     "start_time": "2019-08-12T11:47:56.427686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:47:58.020701Z",
     "start_time": "2019-08-12T11:47:57.989250Z"
    }
   },
   "outputs": [],
   "source": [
    "trainable_dataset = pd.DataFrame({'text': X_trainable, 'label': y_trainable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:48:01.220390Z",
     "start_time": "2019-08-12T11:47:59.776868Z"
    }
   },
   "outputs": [],
   "source": [
    "trainable_dataset.to_csv(\"./trainable_data.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:48:02.732118Z",
     "start_time": "2019-08-12T11:48:02.728173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Analysis**\n",
    "Before we start, we want to do a bit of exploratory analysis on the data. Specifically we need\n",
    "to know how many unique words there are in the corpus and how many words are there in\n",
    "each sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming language,Designed by,Appeared,Extension <br>\n",
    "Python,Guido van Rossum,1991,.py <br>\n",
    "Java,James Gosling,1995,.java <br>\n",
    "C++,Bjarne Stroustrup,1983,.cpp <br>\n",
    "\n",
    "================================\n",
    "row will be: <br>\n",
    "['Programming language; Designed by; Appeared; Extension'] <br>\n",
    "['Python; Guido van Rossum; 1991; .py'] <br>\n",
    "['Java; James Gosling; 1995; .java'] <br>\n",
    "['C++; Bjarne Stroustrup;1983;.cpp'] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:49:07.903662Z",
     "start_time": "2019-08-12T11:48:06.498043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manishgarg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "max_length = 0\n",
    "embedding_dim = 0\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "word_freqs = collections.Counter()\n",
    "num_sentences = 0\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "with open(\"./trainable_data.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentence = row[1]\n",
    "        label = row[2]\n",
    "        words = nltk.word_tokenize(sentence, language='english')\n",
    "        if len(words) > max_length:\n",
    "            max_length = len(words)\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        num_sentences += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the number of unique words len(word_freqs), we set our vocabulary size to a\n",
    "fixed number and treat all the other words as out of vocabulary (OOV) words and replace\n",
    "them with the pseudo-word UNK (for unknown). At prediction time, this will allow us to\n",
    "handle previously unseen words as OOV words as well. <br>\n",
    "\n",
    "The number of words in the sentence (max_length) allows us to set a fixed sequence length and\n",
    "zero pad shorter sentences and truncate longer sentences to that length as appropriate.\n",
    "\n",
    "Even though RNNs handle variable sequence length, this is usually achieved either by padding\n",
    "and truncating as above, or by grouping the inputs in different batches by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:30:33.209427Z",
     "start_time": "2019-08-12T12:30:33.205049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences: 160000\n",
      "max_length: 1239\n",
      "word_count: 167\n"
     ]
    }
   ],
   "source": [
    "word_count = len(word_freqs)\n",
    "print(\"num_sentences:\",num_sentences)\n",
    "print(\"max_length:\",max_length)\n",
    "print(\"word_count:\",word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T08:50:07.317217Z",
     "start_time": "2019-08-11T08:50:07.163776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 2 columns):\n",
      "label    1599999 non-null int64\n",
      "text     1599999 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "trainable_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:49:59.076440Z",
     "start_time": "2019-08-12T11:49:59.073265Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = trainable_dataset['text'].values\n",
    "ys = trainable_dataset['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T08:53:00.447749Z",
     "start_time": "2019-08-11T08:53:00.439080Z"
    }
   },
   "source": [
    "**First spliting of total data to train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:12.110202Z",
     "start_time": "2019-08-12T11:52:12.029087Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=101,\n",
    "                                                    stratify=ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:13.184458Z",
     "start_time": "2019-08-12T11:52:13.180476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:14.220420Z",
     "start_time": "2019-08-12T11:52:14.216312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:14.848187Z",
     "start_time": "2019-08-12T11:52:14.844303Z"
    }
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:15.408394Z",
     "start_time": "2019-08-12T11:52:15.404399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:15.958515Z",
     "start_time": "2019-08-12T11:52:15.954272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16000, 16000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:52:17.321294Z",
     "start_time": "2019-08-12T11:52:17.318323Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ohOGz24lsNAD"
   },
   "outputs": [],
   "source": [
    "# sentences=[]\n",
    "# labels=[]\n",
    "# random.shuffle(corpus)\n",
    "# for x in range(training_size):\n",
    "#     sentences.append(corpus[x][0])\n",
    "#     labels.append(corpus[x][1])\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# vocab_size=len(word_index)\n",
    "\n",
    "# sequences = tokenizer.texts_to_sequences(sentences)\n",
    "# padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# split = int(test_portion * training_size)\n",
    "\n",
    "# test_sequences = padded[0:split]\n",
    "# training_sequences = padded[split:training_size]\n",
    "# test_labels = labels[0:split]\n",
    "# training_labels = labels[split:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:06.951409Z",
     "start_time": "2019-08-12T12:54:04.856950Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_length, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:06.955936Z",
     "start_time": "2019-08-12T12:54:06.953211Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:07.110243Z",
     "start_time": "2019-08-12T12:54:07.106992Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gIrtRem1En3N",
    "outputId": "0eddccce-7cc6-47fc-d6b0-a6ffbded69f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(word_index['i'])\n",
    "# Expected Output\n",
    "# 138858\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:10.284212Z",
     "start_time": "2019-08-12T12:54:10.281563Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "C1zdgJkusRh0",
    "outputId": "319c0302-bcef-4b74-8a09-6fb290fa151c"
   },
   "outputs": [],
   "source": [
    "# Note this is the 100 dimension version of GloVe from Stanford\n",
    "# # I unzipped and hosted it on my site to make this notebook easier\n",
    "# !wget --no-check-certificate \\\n",
    "#     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n",
    "#     -O /tmp/glove.6B.100d.txt\n",
    "# embeddings_index = {};\n",
    "# with open('/tmp/glove.6B.100d.txt') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split();\n",
    "#         word = values[0];\n",
    "#         coefs = np.asarray(values[1:], dtype='float32');\n",
    "#         embeddings_index[word] = coefs;\n",
    "\n",
    "# embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word);\n",
    "#     if embedding_vector is not None:\n",
    "#         embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:11.016990Z",
     "start_time": "2019-08-12T12:54:11.014726Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "71NLk_lpFLNt",
    "outputId": "fa423a6f-23c8-43ec-e224-39936a09d951"
   },
   "outputs": [],
   "source": [
    "# print(len(embeddings_matrix))\n",
    "# # Expected Output\n",
    "# # 138859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:11.657221Z",
     "start_time": "2019-08-12T12:54:11.654615Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "iKKvbuEBOGFz",
    "outputId": "a4e4dd1c-3cfd-4d80-9698-6ecabe0a3943"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "#     tf.keras.layers.LSTM(64),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# num_epochs = 50\n",
    "# history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)\n",
    "\n",
    "# print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:17.002426Z",
     "start_time": "2019-08-12T12:54:13.143303Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences,maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:54:18.786825Z",
     "start_time": "2019-08-12T12:54:18.055244Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:59:48.836452Z",
     "start_time": "2019-08-12T12:59:48.358534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1239, 128)         17920     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 1239, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 67,393\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,EMBEDDING_SIZE, input_length=max_length,\n",
    "                                    embeddings_initializer=\"glorot_uniform\"))\n",
    "model.add(tf.compat.v1.keras.layers.SpatialDropout1D(rate=0.2))\n",
    "model.add(tf.keras.layers.LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:59:51.135472Z",
     "start_time": "2019-08-12T12:59:51.131416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 1239)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-12T12:59:52.381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128000 samples, validate on 32000 samples\n",
      "WARNING:tensorflow:From /Users/manishgarg/anaconda3/envs/udacity/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "  3008/128000 [..............................] - ETA: 4:51:55 - loss: 0.6945 - acc: 0.5156"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, \n",
    "                    validation_data=(testing_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-12T13:00:14.796Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qxju4ItJKO8F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.plot(epochs, val_acc, 'b')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.plot(epochs, val_loss, 'b')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Expected Output\n",
    "# A chart where the validation loss does not increase sharply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP Course - Week 3 Exercise Answer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
